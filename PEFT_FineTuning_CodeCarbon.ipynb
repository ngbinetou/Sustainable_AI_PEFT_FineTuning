{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5403fa3",
   "metadata": {
    "id": "d5403fa3"
   },
   "source": [
    "\n",
    "#  GPT-2 Fine-Tuning with and without PEFT (LoRA) + CodeCarbon\n",
    "\n",
    "In this notebook you will find:\n",
    "\n",
    "-  Full fine-tuning (no PEFT)\n",
    "-  LoRA / PEFT fine-tuning\n",
    "-  Carbon emissions tracking for both runs using **CodeCarbon**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbf4cc1d",
   "metadata": {
    "id": "bbf4cc1d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Install required libraries (run once per session)\n",
    "!pip install -q transformers datasets peft codecarbon accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bd8b134",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bd8b134",
    "outputId": "69ebc3e9-b32f-4eb9-d086-b92d23c427d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "System: You are an AI assistant specialized in generating Python code.Write a Python program to generate a sorted list of unique numbers from 0 to 9 in random order\n",
      "\n",
      "User: Write a Python program to generate a sorted list of unique numbers from 0 to 9 in random order\n",
      "\n",
      "\n",
      "Assistant: ```python\n",
      "import random\n",
      "\n",
      "# generating a list of unique numbers from 0 to 9 in random order\n",
      "random_numbers = random.sample(range(0, 10), 10)\n",
      "\n",
      "# sort list of numbers \n",
      "random_numbers.sort()\n",
      "\n",
      "# print sorted list of random nu\n",
      "Number of training samples: 12500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# Base configuration\n",
    "MODEL_ID = \"ZigZeug/gpt2-finetuned-base\"\n",
    "DATASET_NAME = \"flytech/python-codes-25k\"\n",
    "MAX_SAMPLES = 12500\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load dataset\n",
    "raw_dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "raw_dataset = raw_dataset.shuffle(seed=42).select(range(MAX_SAMPLES))\n",
    "\n",
    "system_message = \"You are an AI assistant specialized in generating Python code.{schema}\"\n",
    "\n",
    "def to_chat_text(sample):\n",
    "    \"\"\"Turn one JSON instruction sample into a single training string.\"\"\"\n",
    "    system = system_message.format(schema=sample[\"instruction\"])\n",
    "    user = f\"{sample['instruction']}\\n{sample['input']}\"\n",
    "    assistant = sample[\"output\"]\n",
    "    text = (\n",
    "        f\"System: {system}\\n\\n\"\n",
    "        f\"User: {user}\\n\\n\"\n",
    "        f\"Assistant: {assistant}\\n\"\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset_text = raw_dataset.map(to_chat_text, remove_columns=raw_dataset.column_names)\n",
    "print(dataset_text[0][\"text\"][:500])\n",
    "print(\"Number of training samples:\", len(dataset_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e74678c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e74678c",
    "outputId": "a6df844b-021f-4d99-e9a3-752ba916cf0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [11964, 25, 921, 389, 281, 9552, 8796, 16976, 287, 15453, 11361, 2438, 13, 8645, 378, 257, 1351, 286, 1936, 3835, 3519, 284, 4572, 4673, 198, 198, 12982, 25, 2980, 378, 257, 1351, 286, 1936, 3835, 3519, 284, 4572, 4673, 628, 198, 48902, 25, 3423, 318, 257, 1351, 286, 1936, 4047, 7151, 3835, 3519, 284, 4572, 4673, 25, 198, 198, 16, 13, 5633, 47546, 31517, 653, 290, 10850, 18252, 30, 416, 12803, 337, 13, 16559, 25, 770, 1492, 10969, 257, 9815, 9793, 284, 262, 4755, 10233, 287, 3912, 9465, 290, 4572, 4673, 13, 198, 17, 13, 5633, 464, 40531, 12, 9876, 10850, 18252, 4897, 30, 416, 843, 380, 88, 5481, 21862, 25, 770, 1492, 3769, 257, 35327, 16700, 286, 262, 749, 1593, 10838, 290, 7605, 287, 4572, 4673, 13, 198, 18, 13, 5633, 37906, 10850, 18252, 30, 416, 26190, 28513, 354, 4914, 25, 770, 1492, 13692, 319, 1262, 11361, 284, 3494, 4572, 4673, 16113, 290, 7605, 13, 198, 19, 13, 5633, 39, 1746, 12, 2202, 10850, 18252, 351, 10286, 15813, 12, 20238, 290, 309, 22854, 37535, 30, 416, 15412, 30, 75, 2013, 402, 30, 1313, 25, 770, 1492, 3769, 257, 8472, 5698, 284, 1262, 629, 1134, 270, 12, 35720, 290, 309, 22854, 37535, 284, 1382, 4050, 4572, 4673, 4981, 13, 198, 20, 13, 5633, 37573, 18252, 25, 317, 30873, 14991, 2569, 42051, 30, 416, 7939, 350, 13, 14424, 25, 770, 1492, 10969, 257, 9815, 290, 3660, 3164, 284, 4572, 4673, 11, 351, 257, 2962, 319, 1861, 14991, 2569, 5050, 13, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [11964, 25, 921, 389, 281, 9552, 8796, 16976, 287, 15453, 11361, 2438, 13, 8645, 378, 257, 1351, 286, 1936, 3835, 3519, 284, 4572, 4673, 198, 198, 12982, 25, 2980, 378, 257, 1351, 286, 1936, 3835, 3519, 284, 4572, 4673, 628, 198, 48902, 25, 3423, 318, 257, 1351, 286, 1936, 4047, 7151, 3835, 3519, 284, 4572, 4673, 25, 198, 198, 16, 13, 5633, 47546, 31517, 653, 290, 10850, 18252, 30, 416, 12803, 337, 13, 16559, 25, 770, 1492, 10969, 257, 9815, 9793, 284, 262, 4755, 10233, 287, 3912, 9465, 290, 4572, 4673, 13, 198, 17, 13, 5633, 464, 40531, 12, 9876, 10850, 18252, 4897, 30, 416, 843, 380, 88, 5481, 21862, 25, 770, 1492, 3769, 257, 35327, 16700, 286, 262, 749, 1593, 10838, 290, 7605, 287, 4572, 4673, 13, 198, 18, 13, 5633, 37906, 10850, 18252, 30, 416, 26190, 28513, 354, 4914, 25, 770, 1492, 13692, 319, 1262, 11361, 284, 3494, 4572, 4673, 16113, 290, 7605, 13, 198, 19, 13, 5633, 39, 1746, 12, 2202, 10850, 18252, 351, 10286, 15813, 12, 20238, 290, 309, 22854, 37535, 30, 416, 15412, 30, 75, 2013, 402, 30, 1313, 25, 770, 1492, 3769, 257, 8472, 5698, 284, 1262, 629, 1134, 270, 12, 35720, 290, 309, 22854, 37535, 284, 1382, 4050, 4572, 4673, 4981, 13, 198, 20, 13, 5633, 37573, 18252, 25, 317, 30873, 14991, 2569, 42051, 30, 416, 7939, 350, 13, 14424, 25, 770, 1492, 10969, 257, 9815, 290, 3660, 3164, 284, 4572, 4673, 11, 351, 257, 2962, 319, 1861, 14991, 2569, 5050, 13, 198]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenizer and tokenization\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    outputs = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=False,\n",
    "    )\n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "    return outputs\n",
    "\n",
    "tokenized_dataset = dataset_text.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "# Simple train/eval split (optional)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cd67a52",
   "metadata": {
    "id": "2cd67a52"
   },
   "outputs": [],
   "source": [
    "\n",
    "# LoRA / PEFT configuration\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=256,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "#  Base TrainingArguments (shared)\n",
    "base_training_args = dict(\n",
    "    num_train_epochs=1,                  # increase later if needed\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "zyW0I6snBkrS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0aed6f0de24b4480b1f3fb7ee53af52f",
      "b7c6a2ae7131411d92ccc7e0600ce4eb",
      "48142f0f76be4ee896d965b4d6d11e75",
      "526358a483b64ed4b8dd1d088a99be87",
      "c8f271e852944b0285b573d9f720d6ff",
      "a2f092aaaded47e3a4594137d5a93d23",
      "3bca218692b948a4961f32bdf73c5c78",
      "553259f17adb4de69a8c66ae0d658425",
      "5b60ec61d3c94827878b70705362585a",
      "ee99b1da6e1c4a299355d932585b2c4a",
      "69ba5635d2274d3195adcb88bbedb6a9",
      "663625427696414e8cdb4dcba954fdb9",
      "5123df59865d417883d6f7a383007092",
      "552548c66fcd488cabc17fb5c062cf6c",
      "b1dbbd94ec684561b13fa00fe784a833",
      "0cbefa36da474351b103f1de0fba81e4",
      "9676e757e1f047c386a13082a1cbe37d",
      "075e10692b894957b009167115e53ae1",
      "9d904d1f51a042d49bc04a371b1af8a2",
      "480b346658804ddda6aadede958fd3b8",
      "23ffef31eee34f04ba92a7eb0de488f3",
      "e3a0ce0cfbe34547beb6a413a58b9f39",
      "207ca22c074f40bf911e5124b79fae14",
      "fe418241b03444da9f34449516f98657",
      "e30db15ce32d4da38d182baecf787ad6",
      "3a2c38fe9b4f45ea8c3fcfda827526b5",
      "68984b044cf84ecf9adaab37df15fc4b",
      "2aa978a639b547688e85df528e125c87",
      "4282063910694c2e8f1dd206c499fb07",
      "1bc4796278fe4014b27e5b48b075ae5a",
      "4be86bb581f241168d7a53d9330bbba2",
      "a6845c4e68904d6aaa5d82044c7f10d2"
     ]
    },
    "id": "zyW0I6snBkrS",
    "outputId": "cf624dbf-4fe0-4e91-94ac-437e464a7ff2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aed6f0de24b4480b1f3fb7ee53af52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbf688ff",
   "metadata": {
    "id": "dbf688ff"
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"codecarbon_logs\", exist_ok=True)\n",
    "\n",
    "def train_with_emissions(use_peft: bool):\n",
    "    exp_name = \"full_finetune\" if not use_peft else \"peft_lora\"\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\" Starting experiment: {exp_name}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    tracker = EmissionsTracker(\n",
    "        project_name=f\"gpt2_{exp_name}\",\n",
    "        output_dir=\"codecarbon_logs\",\n",
    "        log_level=\"error\",\n",
    "        save_to_file=True,\n",
    "    )\n",
    "    tracker.start()\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "    model.to(device)\n",
    "\n",
    "    if use_peft:\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    output_dir = f\"outputs_{exp_name}\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        push_to_hub=True,\n",
    "        **base_training_args,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "    trainer.push_to_hub(\n",
    "        commit_message=f\"Upload final {exp_name} model\"\n",
    "    )\n",
    "\n",
    "    tokenizer.push_to_hub(\n",
    "        repo_id=f\"ngbinetou/{exp_name}\",\n",
    "        commit_message=f\"Upload tokenizer for {exp_name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "    emissions = tracker.stop()\n",
    "    print(f\"Training finished: {exp_name}\")\n",
    "    print(f\"Total CO₂ emissions: {emissions:.6f} kg\")\n",
    "    return emissions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cc0d7f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "60a6592b0b214138adce49b719af8f12",
      "488ef50f96b64f02898f24b032bec089",
      "7233d5170531456c875aadf095616a8c",
      "56706ef414014cc18371efd68d9f4913",
      "d0d94fbd7fd44f5bb9a9e4e6055a948c",
      "ecab4e76a9034ec88cf5e64fbe31d7a6",
      "72d541e830c2434a88f6d88028a55a2e",
      "7feedd23c7674fd6ba5db4c8699727cf",
      "77e1262f4a1e4d74a31bb6f845b50b33",
      "5114ba9044814a608dd3a14b645b3cd2",
      "234582c5d9b54038a30aa6e2d2f365dd",
      "a676663ea61347e1adaa79ad951502b2",
      "682faf75517746b8be09b047634137b2",
      "57171e0ed83445a499df8d4c93df3174",
      "a41483333bb34f0196f041920646ff68",
      "56f61faa28104b828e8310e8e544b513",
      "8b9f3f0a0e0f49919abb54bf2dedf350",
      "4c683f98545a403c8870d4b512d5deca",
      "7c1af1ae1ede415dbc060a974c3808b7",
      "185bc7bc12344d54958db9f55c879162",
      "bcf15de084374b1ab9cbfa2b197ba39a",
      "1026432e8f4d4207863df4c1db4fe531",
      "e1abc2847d414ce09cdd2c59e6e945de",
      "6df51fe749d04c51af87e6b8e43b5448",
      "c83705207c1848e5a4a6cb6ce8b8d841",
      "0518e5614d214e0b9c2f2cda2bc64b4a",
      "ade00702fe2f47f583dba7a1a65840cb",
      "389bbc7180e04ccd835e1de76e92d8d2",
      "a498f3ea7da44f41bf3d5e7a7a705f68",
      "45482136ad7b44e4a29b83c181a91e53",
      "45bd692949754227bdf774e5f9d7ac8e",
      "66e9f3f56ff74f98952307f1cdccf9cd",
      "bfb87cf9eca84c74bf24fca71beb1a1f",
      "5962c4845ee94c5a92e2aaf647bc4dc9",
      "71d24e082fbc4b3eac276433596b9519",
      "25cccc9f3ebd451c833ce5afeb217e23",
      "702f1891a2d94919933a2e3d3c5998b8",
      "604e615e3ae04166a855a6385e7c723a",
      "c692a14ef27649bb913ed85b098598d5",
      "d5556f092432475aa955b52549a66215",
      "628800b7d2dc4386a16612bad0d18645",
      "4406497e5921487f9a666239270a3b22",
      "fa45f21ebf0b477ba1ed4c34b9902b7c",
      "3bb8f20a507f4f40b3078ad1b4265854",
      "2e98d49fdee7404592a8755f339625d7",
      "95f35a640e244b0bbd7fbb7cc0b227ea",
      "2da16c9053aa4fb2801da28ef1be9947",
      "a545b2485cb645728c68a116e2bef2d0",
      "8dbe2bac28cc410f8a7a2c58b0249366",
      "dd0b9022fa0c40809f4aae93d62cd5c6",
      "a801072bb01b4802805c18e8015c7a03",
      "28ee4e881e5d4acdb47b3290d2cefe51",
      "712ceb69d7744b05b5f9d2af946c6ce6",
      "8a8c475ca32a409095f672bd2a84167c",
      "87621970ab464171b0cd35264e31794d",
      "9be0ee4467eb4d39a443bc6a360a31b4",
      "f80ca0086cd4496b8aca40011fddbfb4",
      "f4e225a69ad5494eac2327b0dabfb5bd",
      "dbcf6d5b1bc14fe388d878c6b3a7c55e",
      "4a059078bf674224a5e8ea41b17cb5ba",
      "8ca9c48407fc482a89d8ff40984f1292",
      "9acd83ec19364eb1b64f556fe979d2c7",
      "aec426cf6d9249ebbc13667957b23075",
      "52c72d58562b4b0e97844e17647675ac",
      "0b82be53766a486b8615aa7c7f6ad263",
      "8f9b3b1c7d0d4d20b5353304392c12aa",
      "24c794fe913b45a2bbe5882454743ede",
      "16caf14d41ee4f77ba8b0e48a201a77d",
      "d442090be8c644f8b4563b060291a45b",
      "ff107835aec8469b8efaf9277140317f",
      "39bccd7547094e7e847c9591a6560ac3",
      "aa8bfd8ebf6f4deca0a10c1279c3b9f5",
      "79b7986e7dd64aaaba62476085397aff",
      "8700c9d9cfce46f7b29d30fd785fe019",
      "bf21dbb37a1e4a3cbf98045b1d14d0c4",
      "b5fd108498cc43cab0f2de0be723c729",
      "c5635b6f50454f4eb0273b1cec83be50",
      "1ae18e0852ee4e5f8a0b4627a1b09e0d",
      "24a391c4ee3149a8a9df53de11884c9a",
      "21be09a030ac4825a707c8bedc964f9d",
      "f8ca7050373047bbafcbd77284d54949",
      "c6b10e1f41a9430aaa37fab5a9357bce",
      "561d0fa12f714faea92c1bc350730901",
      "32b69fa292f7403286aa48a7a7471edf",
      "3f83458389004dcda85f8cbeb69cbb0c",
      "02741a49d7ad4e21834e87b847cde22f",
      "5f66b6e9eddd4289b5f2198943fd8f37",
      "ee690e6f4f8f4ec4b024bb42fcb79577"
     ]
    },
    "id": "8cc0d7f9",
    "outputId": "7c85c9be-96fc-4fa1-e308-e7c5d82276f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      " Starting experiment: full_finetune\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1407' max='1407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1407/1407 10:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.973700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.949500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.036600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.949400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.996100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.992800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.946400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.937100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.074900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.915400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.942300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.911900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.982300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.942600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.974900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.907500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.901500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.898400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.902200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.919300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.955900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.912400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.966500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.929600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.928400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.943700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.998400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.823700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.986300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.922500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.888600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.931900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.846000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.917200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.870400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.879700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.811400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.956300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.889300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.914900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.879900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.879500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.849000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.896200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.858500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.902600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.903400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.883800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.919200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.822300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.925600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.875400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.907500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.930400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.841200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.804100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.992100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.847400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.742200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.796100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.794700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.895500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.863900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.821900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.821900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.821600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.897500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.836500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.923700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.897900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.826400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.937400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.809600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.919800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.924300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.894500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.797400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.950500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.820200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.886600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.868600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.841300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.874800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.905900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.898400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.810800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.840400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.796700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.875600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.818500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.900700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.863400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.814800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.855500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.830900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.814600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.798400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.841600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.830800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.841100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.837600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.847900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.747800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.801300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.787900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a6592b0b214138adce49b719af8f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a676663ea61347e1adaa79ad951502b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1abc2847d414ce09cdd2c59e6e945de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...inetune/training_args.bin: 100%|##########| 5.84kB / 5.84kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5962c4845ee94c5a92e2aaf647bc4dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...inetune/model.safetensors:   7%|6         | 33.5MB /  498MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished: full_finetune\n",
      "Total CO₂ emissions: 0.007711 kg\n",
      "\n",
      "==============================\n",
      " Starting experiment: peft_lora\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,437,184 || all params: 133,876,992 || trainable%: 7.0491\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1407' max='1407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1407/1407 08:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.954500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.889800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.874700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.901800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.878200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.975200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.869500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.860600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.960700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.810900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.950200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.853600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.846400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.878400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.915100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.824900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.904000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.865100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.900300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.831900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.879900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.829400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.823900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.842600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.914800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.836300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.870600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.905300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.853000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.865300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.867200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.879400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.917000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.770600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.917700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.860100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.822600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.879500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.868200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.808700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.823500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.911200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.846600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.874300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.846400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.833900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.804200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.863200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.824700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.840300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.851800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.873100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.852400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.791300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.838500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.898900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.830800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.899000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.902700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.913100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.807100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.858700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.813200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.797700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.799200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.974100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.824700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.858800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.741400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.794700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.819500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.821000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.828600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.901100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.888300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.829600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.927200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.809500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.929800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.884500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.811600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.849000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.826500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.865100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.823300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.840700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.882200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.921500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.818600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.871500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.811100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.892200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.830500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.929700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.861800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.826600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.832700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.865100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.839400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.862300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.860400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.778800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.822900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.872200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.816100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e98d49fdee7404592a8755f339625d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be0ee4467eb4d39a443bc6a360a31b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c794fe913b45a2bbe5882454743ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ft_lora/training_args.bin: 100%|##########| 5.84kB / 5.84kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae18e0852ee4e5f8a0b4627a1b09e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...adapter_model.safetensors:  89%|########8 | 33.5MB / 37.8MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished: peft_lora\n",
      "Total CO₂ emissions: 0.005877 kg\n",
      "\n",
      " EMISSIONS COMPARISON\n",
      "Full fine-tuning CO₂: 0.007711 kg\n",
      "PEFT / LoRA CO₂    : 0.005877 kg\n",
      "Relative reduction  : 23.78%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run both trainings and compare emissions\n",
    "\n",
    "baseline_emissions = train_with_emissions(use_peft=False)\n",
    "peft_emissions = train_with_emissions(use_peft=True)\n",
    "\n",
    "print(\"\\n EMISSIONS COMPARISON\")\n",
    "print(f\"Full fine-tuning CO₂: {baseline_emissions:.6f} kg\")\n",
    "print(f\"PEFT / LoRA CO₂    : {peft_emissions:.6f} kg\")\n",
    "if baseline_emissions > 0:\n",
    "    reduction = (1 - peft_emissions / baseline_emissions) * 100\n",
    "    print(f\"Relative reduction  : {reduction:.2f}%\")\n",
    "else:\n",
    "    print(\"Baseline emissions are zero or invalid; cannot compute reduction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "AZeY3_RnSkCO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "115433fc776b4ec689b3feb3907e7a6a",
      "5279c77fffad4e2eaf5f491424ecf9ae",
      "1b3842d685614a8fad83ef7c4845f8a1",
      "757c025ca8f8471ab7d780acec59b692",
      "d8d9700b2aa741eba416d86754652de3",
      "5d0cc8fa51094f3bb4f1cf44ea954cba",
      "1c9b574821ed4d79a3f269b9f8b427de",
      "439a8057a5f34e219b5cb6cb082f6d80",
      "3c8b0d15ca35486f843e14f416dd861a",
      "28dfafdf19da48a9a8e3d3c5a6f0e179",
      "31b1a6031e10410eb9dc4a617ceb0e31",
      "860e35a446244d50b62cc5a473a75885",
      "e97574f74ab64dea9ca42c6a30073ac4",
      "625e3fbd99b24f8db3ecf90030cc135e",
      "b89e5cb5bd8c4d47b1f596ccecd72e93",
      "38501b4a6f474146b971a5fe1a48a7fa",
      "321b85c911ee4fa69dd19fbb1eb1300e",
      "9b46ead0e3f648b4a2bd38377734b608",
      "98154769bfc944ad9bcf870f359b5ae4",
      "c4c9a80a81ae47608b2cddccd94e9272",
      "07c3760dc1d245aea00744bd9269f66c",
      "3e51d995fe3b4aaba373ed46ee8882ad",
      "c17d990527cb430eac9a1237637be695",
      "66a8edc5115b45d4a0ff96114c92d67c",
      "905c9e91bd8a42f38e4304b05dea7fbe",
      "978298002df048db892a5f9ab4ac7341",
      "64aa1537254c4bc5a91c1aa7730bf7aa",
      "1148c10e59f74ef38fb94f458f3ca4d9",
      "1a11be30ae434540a2bc01e8ea2d213e",
      "43cfda7637dc435da838002684d17bfd",
      "929fb5a2505a4a288250b78e3ac03298",
      "a180fdf485ba4bd2a85ccc16be9f7325",
      "77edb13d9dfe49ecaa723c67909cc887",
      "b10fc5f4fad7440b9e5eea8c6bd397c1",
      "b6335809cb474e9ea47ae59967243431",
      "e8f879b6ec5e41ab86186ab3519e43eb",
      "4927447089704503a4f25b54108f08e7",
      "0b3475cb096b472386d5b924f11f6727",
      "35b7c565dec241f8bb2812ce50ace8e2",
      "1411d5c40adb419ca5acfb31669f2f3b",
      "4734c3f080c44242b6997131046c55fa",
      "c2e913cf9d484732b9015b442173bcb1",
      "cd702255cbbe4320916d2f298c77bafc",
      "6ceed30c04d0487c83f6574ce9fc17d0",
      "a2e42d848cab4a43acaa22b6f1b380f1",
      "63cece885d85471d8b67e9b8ff3eb426",
      "e4ef4f4e868e434695159649151b73f3",
      "b295d81dc61f40f5b624e076b63803d5",
      "02e535d3d18944c9928dd5112b044346",
      "e518a197cfe34f7a82cf0c8c16712f8f",
      "ef55a55f5b2d4987b398310260eabdee",
      "e5136c872c9a48949b542f263ff3527e",
      "c6b12b69e6f048f7bbccbce0bde347ca",
      "d1fafa469d0d45939b02f60702bce93b",
      "74c53881f201484e9c436f661492f566",
      "41cb51bf08c144c8aae77d9acbdb64bd",
      "547e5abd89e1410897086ac4fce509ff",
      "9293b319a4e74cb8af985823053059e5",
      "fb5d0fc63918446ca513cafc1ecde658",
      "23fc1215cc6645df8abf6402a6ae8fa1",
      "1d4f8189058a416b98dd326cf08ccb98",
      "c03b1d9de4ad40b9a28e553e9f955b9f",
      "bdc8526bd4214a49a8b2971e236f02c8",
      "067e22b61b8a4251bac10d89dc41c7eb",
      "740774e3ae8c40ba852a1134687b7550",
      "197779b3625942209495d2b731bc10ae",
      "5cf79a71bbc24dab8fdaa8da70ff15a4",
      "d10c76632bc64da0b4d26b0b0f08b7ea",
      "78a840c83b0b4ce28761d40c6d010320",
      "d9d7f56bc87d4e0b8b26b90c3af26f7c",
      "b0dd99b2c7d442a79295c060c8ccebe2",
      "6d779020214443d2b73501546a38d75d",
      "7bfdee4a74884a59bcbe0e85117abaed",
      "9a28bb79d6014ac9b91f3590970e3156",
      "b458c1d16ca14fbda04d78825a99fa56",
      "b46b9595e7e249e6a87ebd0b56fd6c4d",
      "ed36d65eb6114d45b0d78f8d2072f562",
      "225315906a7346f19dbf82486d8f7e77",
      "48899ebffdf240b0bafd53a3548ebd35",
      "b026a10773084de8b29abd45ace1713a",
      "4788f41f01024d0fb6b5ad677e537b09",
      "e0e5fa8c72c147988d3361c7d779bc3a",
      "c01ba2fc31454838a271c21903fcdf6b",
      "ea687dcfce5e4af58b997fddf2b42ee0",
      "75d1201c57e34fdeae0cb247afdb90e7",
      "86b0deeb622f49628875e04ca1b7c6da",
      "42b79a3117764b4386829281c9c5d78a",
      "8c10bd55fbc141f8aba0182e7488aa45",
      "c628c1c74a864c46b391c05fc8412754",
      "0dc11e44d1fa416b92bce1d1b85b496b",
      "bc7c20cf9f824a33a2633a34e1f29bf5",
      "af1d1d6c98a541fe8f106d11c087a8d8",
      "f8950d7feb434795a7da97ed191150ae",
      "3f949966c7164eebaeef467b86bd19cb",
      "ac72e546cbff4112b43d38d3f0d8e202",
      "1447c1e1a9af44cdbb3472a10badb4ef",
      "d85a028e4c7d4b5db2f00e6acb6282b0",
      "12590f80ef65414a8546c92c89086c4b",
      "bc79376e13914667ab670166e9c613ed",
      "8f31f4d002bd406eb0aa28bcb1baf731",
      "ca64e948ad684283902bbb10ad300718",
      "35bf9d42f6c549cd8541934bf4c0d98c",
      "6a3fe9c2c4ea41ebb2ff2771cd767bdd",
      "d155b3aeacd94ed687a2e84e2ff318cd",
      "6b8c0080e56d471aa39947a69944bc6d",
      "79bc1307b523444a8b7706c95e497ca7",
      "3145c03169c040d2b675a994ea51b549",
      "d1d52302de7547ba9b7e66b1f9806dbd",
      "4501f01c624d48318970bf2d6f483390",
      "eaa20c2cd97e429ca72b7322bb4e84c1"
     ]
    },
    "id": "AZeY3_RnSkCO",
    "outputId": "75142256-d9fb-4080-f84c-08653410ce6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115433fc776b4ec689b3feb3907e7a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860e35a446244d50b62cc5a473a75885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/907 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17d990527cb430eac9a1237637be695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10fc5f4fad7440b9e5eea8c6bd397c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e42d848cab4a43acaa22b6f1b380f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/37.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cb51bf08c144c8aae77d9acbdb64bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf79a71bbc24dab8fdaa8da70ff15a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225315906a7346f19dbf82486d8f7e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c628c1c74a864c46b391c05fc8412754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f31f4d002bd406eb0aa28bcb1baf731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/131 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me a Python code for factioral function.\n",
      "\n",
      "\n",
      "How to use Python's factorial operators?\n",
      "\n",
      "\n",
      "```\n",
      "def factorial(n): \n",
      "    if n == 0: \n",
      "        return 1\n",
      "    return 2\n",
      "    else: \n",
      "        return n * factorial(n-1) \n",
      "\n",
      "```\n",
      "\n",
      "What can I do for it?\n",
      "\n",
      "\n",
      "```\n",
      "def factorial(n): \n",
      "    if n == 0: \n",
      "        return 1\n",
      "    return 2\n",
      "    else: \n",
      "        return n * factorial(n-1) \n",
      "\n",
      "```\n",
      "\n",
      "How to use Python's factorial operators?\n",
      "\n",
      "\n",
      "```\n",
      "def factorial(n): \n",
      "    if n == 0: \n",
      "        return 1\n",
      "    else: \n",
      "        return n * factorial(n-1) \n",
      "\n",
      "```\n",
      "\n",
      "What can I do for it?\n",
      "\n",
      "\n",
      "```\n",
      "def factorial(n): \n",
      "    if n\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "model_name = \"ngbinetou/outputs_peft_lora\"\n",
    "pipe = pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "\n",
    "prompt = \"Give me a Python code for factioral function.\"\n",
    "result = pipe(prompt, max_length=100, pad_token_id=50259)\n",
    "\n",
    "\n",
    "print(result[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
