# -*- coding: utf-8 -*-
"""BCN_FT_peft_codecarbon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wVR5vmp6f9f5R82kza4eeI7NSypwtXhb

#  GPT-2 Fine-Tuning with and without PEFT (LoRA) + CodeCarbon

In this notebook you will find:

-  Full fine-tuning (no PEFT)
-  LoRA / PEFT fine-tuning
-  Carbon emissions tracking for both runs using **CodeCarbon**
"""

# Install required libraries (run once per session)
!pip install -q transformers datasets peft codecarbon accelerate

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model
from codecarbon import EmissionsTracker

# Base configuration
MODEL_ID = "ZigZeug/gpt2-finetuned-base"
DATASET_NAME = "flytech/python-codes-25k"
MAX_SAMPLES = 12500

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

# Load dataset
raw_dataset = load_dataset(DATASET_NAME, split="train")
raw_dataset = raw_dataset.shuffle(seed=42).select(range(MAX_SAMPLES))

system_message = "You are an AI assistant specialized in generating Python code.{schema}"

def to_chat_text(sample):
    """Turn one JSON instruction sample into a single training string."""
    system = system_message.format(schema=sample["instruction"])
    user = f"{sample['instruction']}\n{sample['input']}"
    assistant = sample["output"]
    text = (
        f"System: {system}\n\n"
        f"User: {user}\n\n"
        f"Assistant: {assistant}\n"
    )
    return {"text": text}

dataset_text = raw_dataset.map(to_chat_text, remove_columns=raw_dataset.column_names)
print(dataset_text[0]["text"][:500])
print("Number of training samples:", len(dataset_text))

# Tokenizer and tokenization

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

MAX_LENGTH = 512

def tokenize_function(batch):
    outputs = tokenizer(
        batch["text"],
        truncation=True,
        max_length=MAX_LENGTH,
        padding=False,
    )
    # For causal LM, labels are the same as input_ids
    outputs["labels"] = outputs["input_ids"].copy()
    return outputs

tokenized_dataset = dataset_text.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"],
)

# Simple train/eval split (optional)
split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = split_dataset["train"]
eval_dataset = split_dataset["test"]

print(train_dataset[0])

# LoRA / PEFT configuration

peft_config = LoraConfig(
    r=256,
    lora_alpha=128,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

#  Base TrainingArguments (shared)
base_training_args = dict(
    num_train_epochs=1,                  # increase later if needed
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    logging_steps=10,
    save_strategy="epoch",
    report_to="none",
    fp16=torch.cuda.is_available(),
)

from huggingface_hub import login
login()

os.makedirs("codecarbon_logs", exist_ok=True)

def train_with_emissions(use_peft: bool):
    exp_name = "full_finetune" if not use_peft else "peft_lora"
    print("\n==============================")
    print(f" Starting experiment: {exp_name}")
    print("==============================")

    tracker = EmissionsTracker(
        project_name=f"gpt2_{exp_name}",
        output_dir="codecarbon_logs",
        log_level="error",
        save_to_file=True,
    )
    tracker.start()

    model = AutoModelForCausalLM.from_pretrained(MODEL_ID)
    model.to(device)

    if use_peft:
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()

    output_dir = f"outputs_{exp_name}"

    training_args = TrainingArguments(
        output_dir=output_dir,
        push_to_hub=True,
        **base_training_args,
    )

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )

    trainer.train()


    trainer.push_to_hub(
        commit_message=f"Upload final {exp_name} model"
    )

    tokenizer.push_to_hub(
        repo_id=f"ngbinetou/{exp_name}",
        commit_message=f"Upload tokenizer for {exp_name}"
    )


    emissions = tracker.stop()
    print(f"Training finished: {exp_name}")
    print(f"Total CO₂ emissions: {emissions:.6f} kg")
    return emissions

# Run both trainings and compare emissions

baseline_emissions = train_with_emissions(use_peft=False)
peft_emissions = train_with_emissions(use_peft=True)

print("\n EMISSIONS COMPARISON")
print(f"Full fine-tuning CO₂: {baseline_emissions:.6f} kg")
print(f"PEFT / LoRA CO₂    : {peft_emissions:.6f} kg")
if baseline_emissions > 0:
    reduction = (1 - peft_emissions / baseline_emissions) * 100
    print(f"Relative reduction  : {reduction:.2f}%")
else:
    print("Baseline emissions are zero or invalid; cannot compute reduction.")

"""Exemple de prompt"""

from transformers import pipeline


model_name = "ngbinetou/outputs_peft_lora"
pipe = pipeline("text-generation", model=model_name)


prompt = "Give me a Python code for factioral function."
result = pipe(prompt, max_length=100, pad_token_id=50259)


print(result[0]["generated_text"])